{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic bias\n",
    "### Harris Dupre\n",
    "### Summer 2020, Data 612\n",
    "\n",
    "### Do recommender systems reinforce human bias?\n",
    "\n",
    "One of the most difficult things about addressing bias is the fact that people are mostly unaware or minimize their biases. Admitting a bias is like admitting a flaw, and so developers are disincentivized (and maybe less able) of seeing bias in their own system.\n",
    "\n",
    "Recommender systems, at least in the context of this class, are logical algorithms that show no preference toward one result or another. I suppose there's some possibility that bias could be built in to the algorithm, maybe if it was over-fitted to a certain set of data while it was being created. But really, any decent recommendation algorithm should be tested for functionality on a multitude of datasets.\n",
    "\n",
    "The most dangerous and persistent source of bias that will occur in the context of recommneder systems will be from a user. Human interpretation of recommender results might have bias, and the data gathered and fed to the system will come from have biased people.\n",
    "\n",
    "### Unethical uses of recommender systems\n",
    "\n",
    "Recommender systems do reinforce unethical targeting and customer segmentation mostly because businesses purposely use them in that way to gain a competitive advantage.\n",
    "\n",
    "Target was famously able to tell when a customer was pregnant before the customer knew themselves -- and Target promptly served up baby-related ads in response.\n",
    "\n",
    "This article is about Facebook's targeting of a \"Jew-hater\" customer segment for advertising:\n",
    "https://www.theatlantic.com/technology/archive/2017/09/on-facebook-advertisers-can-show-their-ads-only-to-jew-haters/539964/\n",
    "\n",
    "Even if Facebook's algorithm innocently, using objective data, identified a user who was likely to be a Jew-hater the main problem here is with the decision about what to do with this data.\n",
    "\n",
    "By allowing advertisers to target those users and serve them products and content they created a more tangible community of those users that share deeper interests. Now a loose collection of prejudiced people are shopping at the same stores and reading from the same new sources. Why this is not a good idea for certain customer segments should not have to be explained -- but for companies it's clearly about profit over morality.\n",
    "\n",
    "Facebook didn't have to go back to the drawing board to reduce bias in their algorithm and make sure no racist or sexist or homophobic customer segments ever appear again. An appropriate response would have been to prevent advertisers from targeting that customer segment, and then maybe start to wonder why so many Jew-haters are using your site."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
